/*
 *  Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
 *  This source file is part of the Cangjie project, licensed under Apache-2.0
 *  with Runtime Library Exception.
 *
 *  See https://cangjie-lang.cn/pages/LICENSE for license information.
 */

package stdx.string_intern

/**
 * @brief  LRU cache implementation:
 *
 * The bidirectional linked list is used to store cache keys.
 * In the LRU, the header data of the linked list indicates that the linked list has been recently accessed, and the tail data indicates that the linked list has been recently accessed.
 * The tail data is preferentially eliminated. After the cache is hit, the hit key needs to be placed in the header of the linked list. When the cache is updated, New data is also placed in the cache header.
 */
class LRUMemoryCache<K, V> <: BaseDoubleLinkedListMemoryCache<K, V> where K <: Hashable & Equatable<K> {
    public init(cacheConfigs!: ?ICacheConfig, cacheKeyLinkList!: ILinkList<K> = ConcurrentSkipingLinkList<K>()) {
        super(cacheConfigs: cacheConfigs, cacheKeyLinkList: cacheKeyLinkList)
    }

    /**
     * After cache hit processing, the LRU algorithm needs to place the cache key in the queue header.
     *
     * @param cacheObj Current hit cache
     */
    protected func adjustCacheAfterHit(cacheObj: ICacheObj<K, V>): Unit {
        super.cacheKeyLinkList.makeFirst(cacheObj.key)
    }

    /**
     * The LRU algorithm needs to place the cache key in the queue header for post-update processing.
     *
     * @param cacheObj Indicates the cache that is updated or added to the current cache.
     */
    protected func adjustCacheAfterPut(cacheObj: ICacheObj<K, V>, oldObj!: ?ICacheObj<K, V>): Unit {
        super.cacheKeyLinkList.prepend(cacheObj.key)
    }
}
